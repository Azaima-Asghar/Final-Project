# *Modeling Probablity Of Default*

## *Group Name: Team 7*

## *Group Members*

Azaima Azghar: Square Role; Repository Manager.

Colin Wallace: Triangle role; Machine Learning Architect.

Kalkidan Alemayehu: Circle role; Database Manager.

Osama Ali: X role; Technology Manager.

## *Selected topic*


Modeling probability of  mortgage application
default.

## *Reason To Select This Topic*

There is a risk in housing market where the applicant of a mortgage application could default on their mortgage payments or the whole loan altogether. Team 7 is going to look into past data of USA mortgage market to see if a common lifestyle or pattern exists to those files which ended up being put in default or bankcrupt status.

## *Resources*

* Data Source: Mortgage Data From Fannie Mae.

    * Acquisition and Performance Data.

* pgAdmin/postgressSQL.
* Google Colabs/Jupyter Notebook.
* Amazon Web Services (AWS).
* Github.
* Visual Studio Code.
* Microsoft Excel.

PLAN: The acquisition data and the performance data are merged on common feature 'LOAN IDENTIFIER'.We want to get more satasets for mortgage and merge them as well down the road to include macro variables such as employment and unemployment.

## *Description of the communication protocols.*

Team 7 has a shared slack channel that incluces all four members of the team: Colin Wallace (Triangle role), Kalkidan Alemayehu (Circle role), Osama Ali (X role ) and Azaima Asghar (Square Role). We communicate through this channel and all four of us are very active on this channel, we discuss our progress and our difficulties with others. We also do zoom meetings every other day or when it is required to meet to discuss the next steps. Everyone is happy to help each other if someone faces some issue. Going forward we plan to learn plotly dash to create the dashboard for our project, since we will be learning something new we all will be trying to understand it and work together to create the dashboard. 

Starting next week we will clean the present data properly and also look for other datasets that will contain additional macro variables. All in all with our roles dututies we all plan to code and help each other get the job done early and leave some addtional time to go over our work and try to make it even better.